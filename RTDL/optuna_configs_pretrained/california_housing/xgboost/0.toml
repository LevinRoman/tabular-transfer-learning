program = 'bin/xgboost_.py'

[base_config]
seed = 0

    [base_config.data]
    dset_id = 'california_housing' # id of openml dataset
	task = 'regression' # can be binclass, multiclass, regression
	cat_policy = 'ohe'
	y_policy = 'mean_std'

    [base_config.fit]
    early_stopping_rounds = 50
    verbose = false

    [base_config.model]
    booster = 'gbtree'
    n_estimators = 2000
    n_jobs = -1
    tree_method = 'gpu_hist'

    [base_config.transfer]
	stage = 'pretrain'
	load_checkpoint = false
	checkpoint_path = nan
	pretrain_proportion = 0.5
	downstream_samples_per_class = nan
	freeze_feature_extractor = false
	layers_to_fine_tune = []
	use_mlp_head = false
	epochs_warm_up_head = 0
	head_lr = nan

[optimization.options]
n_trials = 30

[optimization.sampler]
seed = 0

[optimization.space.model]
alpha = [ '?loguniform', 0, 1e-08, 100.0 ]
colsample_bylevel = [ 'uniform', 0.5, 1.0 ]
colsample_bytree = [ 'uniform', 0.5, 1.0 ]
gamma = [ '?loguniform', 0, 1e-08, 100.0 ]
lambda = [ '?loguniform', 0, 1e-08, 100.0 ]
learning_rate = [ 'loguniform', 1e-05, 1 ]
max_depth = [ 'int', 3, 10 ]
min_child_weight = [ 'loguniform', 1e-08, 100000.0 ]
subsample = [ 'uniform', 0.5, 1.0 ]