program = 'bin/mlp.py'

[base_config]
seed = 0

	[base_config.data]
	dset_id = 'year' # id of openml dataset
	task = 'regression' # can be binclass, regression, regression
	cat_policy = 'indices'
	normalization = 'quantile'
	y_policy = 'mean_std'

	[base_config.model]
	[base_config.training]
	batch_size = 256
	eval_batch_size = 8192
	n_epochs = 500
	optimizer = 'adamw'
	patience = 30
	lr_n_decays = 0
	num_batch_warm_up = 0

	[base_config.transfer]
	stage = 'pretrain'
	load_checkpoint = false
	checkpoint_path = nan
	pretrain_proportion = 0.5
	downstream_samples_per_class = nan
	freeze_feature_extractor = false
	layers_to_fine_tune = []
	use_mlp_head = false
	epochs_warm_up_head = 0
	head_lr = nan

[optimization.options]
n_trials = 30

[optimization.sampler]
seed = 0

[optimization.space.model]
d_layers = [ '$mlp_d_layers', 1, 8, 1, 512 ]
dropout = [ '?uniform', 0.0, 0.0, 0.5 ]
d_embedding = ['int', 64, 512]

[optimization.space.training]
lr = [ 'loguniform', 1e-05, 0.01 ]
weight_decay = [ '?loguniform', 0.0, 1e-06, 0.001 ]

