#Need to figure out tokenizer fine-tuning for SAINT
#Do not forget to decrease the lr for downstream?
seed = 0

[data]
dset_id = 1483 # id of openml dataset
task = 'multiclass' # can be binclass, multiclass, regression
cat_policy = 'indices'
normalization = 'quantile'


[transfer]
stage = 'downstream'
load_checkpoint = true
checkpoint_path = '/cmlscratch/vcherepa/rilevin/tabular/tabular-transfer-learning/tabular-transfer-learning/RTDL/output/1483/saint/multiclass_transfer/pretrain/default/0/checkpoint.pt'
pretrain_proportion = 0.5
downstream_train_data_fraction = 0.001
freeze_feature_extractor = true
layers_to_fine_tune = ['tokenizer']#, 'last_normalization', 'norm']

[model]
embed_dim = 128
depth = 1
heads = 4
attn_dropout = 0.1
ff_dropout = 0.1
cont_embeddings = 'MLP'
attentiontype = 'colrow'
final_mlp_style = 'sep'
use_cls = true

[training]
batch_size = 256
eval_batch_size = 256
lr = 0.00001 #could be 1e-5 -- 10x smaller for transfer
lr_n_decays = 0
n_epochs = 1000000000
optimizer = 'adamw'
patience = 16
weight_decay = 0#1e-05
