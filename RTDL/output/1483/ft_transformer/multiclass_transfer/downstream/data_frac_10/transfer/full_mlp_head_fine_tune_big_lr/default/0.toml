#Do not forget to decrease the lr for downstream and possibly freeze layers
#Also, possibly not all weights could be loaded, but that is probably ok

seed = 0

[data]
dset_id = 1483 # id of openml dataset
task = 'multiclass' # can be binclass, multiclass, regression
cat_policy = 'indices'
normalization = 'quantile'


[transfer]
stage = 'downstream'
load_checkpoint = true
checkpoint_path = '/cmlscratch/vcherepa/rilevin/tabular/tabular-transfer-learning/tabular-transfer-learning/RTDL/output/1483/ft_transformer/multiclass_transfer/pretrain/default/0/checkpoint.pt'
pretrain_proportion = 0.5
downstream_samples_per_class = 10
freeze_feature_extractor = false
layers_to_fine_tune = []#['head', 'last_normalization']#, 'norm']
use_mlp_head = true
epochs_warm_up_head = 5
head_lr = 1e-4


[model]
activation = 'reglu'
attention_dropout = 0.2
d_ffn_factor = 1.333333333333333
d_token = 192
ffn_dropout = 0.1
initialization = 'kaiming'
n_heads = 8
n_layers = 3
prenormalization = true
residual_dropout = 0.0

[training]
batch_size = 128
eval_batch_size = 8192
lr = 1e-4 #could be 1e-5 -- 10x smaller for transfer than pretrain
lr_n_decays = 0
n_epochs = 200
optimizer = 'adamw'
patience = 10000
weight_decay = 1e-05
num_batch_warm_up = 0
