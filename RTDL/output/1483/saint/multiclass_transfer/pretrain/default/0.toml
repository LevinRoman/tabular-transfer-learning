#Do not forget to decrease the lr for downstream and possibly freeze layers
#Also, possibly not all weights could be loaded, but that is probably ok

seed = 0

[data]
dset_id = 1483 # id of openml dataset
task = 'multiclass' # can be binclass, multiclass, regression
cat_policy = 'indices'
normalization = 'quantile'


[transfer]
stage = 'pretrain'
load_checkpoint = false
checkpoint_path = nan
pretrain_proportion = 0.5
downstream_samples_per_class = nan
freeze_feature_extractor = false
layers_to_fine_tune = []
use_mlp_head = false
epochs_warm_up_head = 0
head_lr = nan

[model]
embed_dim = 8
depth = 1
heads = 4
attn_dropout = 0.8
ff_dropout = 0.8
cont_embeddings = 'MLP'
attentiontype = 'colrow'
final_mlp_style = 'sep'
use_cls = true

[training]
batch_size = 128
eval_batch_size = 8192
lr = 0.0001
lr_n_decays = 0
n_epochs = 500
optimizer = 'adamw'
patience = 30
weight_decay = 1e-05
num_batch_warm_up = 0
