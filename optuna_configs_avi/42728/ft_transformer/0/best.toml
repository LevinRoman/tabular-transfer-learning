seed = 0

[data]
cat_policy = 'indices'
dset_id = 42728
normalization = 'quantile'
task = 'regression'
y_policy = 'mean_std'

[model]
activation = 'reglu'
attention_dropout = 0.3965328065202336
d_ffn_factor = 0.6892852878683368
d_token = 496
ffn_dropout = 0.05163060857531693
initialization = 'kaiming'
n_heads = 8
n_layers = 4
prenormalization = true
residual_dropout = 0.01655056312432027

[training]
batch_size = 256
eval_batch_size = 8192
lr = 0.0002093664958840757
lr_n_decays = 0
n_epochs = 500
num_batch_warm_up = 0
optimizer = 'adamw'
patience = 30
weight_decay = 7.465489747362422e-05

[transfer]
checkpoint_path = nan
downstream_samples_per_class = nan
epochs_warm_up_head = 0
freeze_feature_extractor = false
head_lr = nan
layers_to_fine_tune = []
load_checkpoint = false
pretrain_proportion = 0.5
stage = 'pretrain'
use_mlp_head = false
