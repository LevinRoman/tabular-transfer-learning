seed = 0

[data]
cat_policy = "indices"
dset_id = 1483
normalization = "quantile"
task = "multiclass"

[model]
activation = "reglu"
attention_dropout = 0.158600871034648
d_ffn_factor = 2.223357630718485
d_token = 344
ffn_dropout = 0.1843625853304821
initialization = "kaiming"
n_heads = 8
n_layers = 2
prenormalization = true
residual_dropout = 0.0

[training]
batch_size = 256
eval_batch_size = 8192
lr = 0.0001
lr_n_decays = 0
n_epochs = 200
num_batch_warm_up = 0
optimizer = "adamw"
patience = 100000.0
weight_decay = 0.0003264635677523522
self_supervised = false

[transfer]
checkpoint_path = nan
downstream_samples_per_class = 50
epochs_warm_up_head = 0
freeze_feature_extractor = false
head_lr = nan
layers_to_fine_tune = []
load_checkpoint = false
pretrain_proportion = 0.5
stage = "downstream"
use_mlp_head = false
